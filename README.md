# Text Quality Classification - MLOps Project

A machine learning system for classifying English text readability levels (Elementary, Intermediate, Advanced) using a fine-tuned BERT model. This project implements a complete MLOps pipeline including training, evaluation, API serving, and cloud deployment.

![Python](https://img.shields.io/badge/Python-3.12-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.6.0-red)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115.6-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

> **âš ï¸ Intel Mac Users:** This project uses PyTorch 2.6.0, which doesn't have wheels for Intel Macs with `uv`. 
> Please see [INTEL_MAC_GUIDE.md](INTEL_MAC_GUIDE.md) for Docker-based setup instructions.

## Made By

**Group 63** - DTU Course 02476 Machine Learning Operations

| Student Number |
|----------------|
| s234869 (Alexander Hougaard) (GitHub: Alexander-bit-boop)|
| s245176 (William Hyldig) (GitHub: Williamhyldig)         |
| s244742 (Valdemar Stamm) (GitHub: HrStamm)               |
| s245362 (Frederik JÃ¸nsson) (GitHub: Trex14)              |
| s246089 (Gustav Christensen) (GitHub: DonConarch)        |

## Overview

This project was developed as part of the **02476 Machine Learning Operations** course at DTU. The goal is to build a production-ready ML pipeline that predicts text readability levels on a 0-2 scale:

- **0**: Elementary (simple vocabulary and sentence structure)
- **1**: Intermediate (moderate complexity)
- **2**: Advanced (complex vocabulary and sentence structure)

### Key Features

- ğŸ¤– **BERT-based model** using the lightweight `prajjwal1/bert-mini` (11M parameters)
- ğŸ“Š **Experiment tracking** with Weights & Biases
- ğŸ³ **Containerized** training and API serving with Docker
- â˜ï¸ **Cloud deployment** on Google Cloud Platform (Cloud Run)
- ğŸ”„ **CI/CD pipelines** with GitHub Actions
- ğŸ“¦ **Data versioning** with DVC and Google Cloud Storage
- ğŸ§ª **Comprehensive testing** with pytest and code coverage
- ğŸ“ˆ **API monitoring** with Prometheus metrics

## Quick Start

For detailed setup instructions, see [QUICKSTART.md](QUICKSTART.md).

```bash
# Clone the repository
git clone <repo-url>
cd ml-ops-assignment

# Install dependencies (requires uv)
uv sync

# Authenticate with Google Cloud (for data/model access)
gcloud auth application-default login

# Pull data and models from DVC
uv run dvc pull

# Evaluate the trained model
uv run invoke evaluate --checkpoint models/model_final.pt

# Start the API server
uv run invoke serve-api
```

## Project Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              Data Pipeline                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   HuggingFace â”‚â”€â”€â”€â”€â–¶â”‚  Tokenizer   â”‚â”€â”€â”€â”€â–¶â”‚  Processed   â”‚                 â”‚
â”‚  â”‚    Dataset    â”‚     â”‚  (BERT)      â”‚     â”‚   Dataset    â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                    â”‚                         â”‚
â”‚                                                    â–¼                         â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                              â”‚         DVC + GCS Storage        â”‚           â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            Training Pipeline                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   Config     â”‚â”€â”€â”€â”€â–¶â”‚   Training   â”‚â”€â”€â”€â”€â–¶â”‚    Model     â”‚                 â”‚
â”‚  â”‚   (YAML)     â”‚     â”‚   Script     â”‚     â”‚  Checkpoints â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                              â”‚                     â”‚                         â”‚
â”‚                              â–¼                     â–¼                         â”‚
â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                       â”‚   W&B      â”‚        â”‚    DVC     â”‚                  â”‚
â”‚                       â”‚  Logging   â”‚        â”‚   Storage  â”‚                  â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Serving Pipeline                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   FastAPI    â”‚â”€â”€â”€â”€â–¶â”‚   Docker     â”‚â”€â”€â”€â”€â–¶â”‚  Cloud Run   â”‚                 â”‚
â”‚  â”‚   Server     â”‚     â”‚   Container  â”‚     â”‚  Deployment  â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                                                                    â”‚
â”‚         â–¼                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                           â”‚
â”‚  â”‚  Prometheus  â”‚                                                           â”‚
â”‚  â”‚   Metrics    â”‚                                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             CI/CD Pipeline                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   GitHub     â”‚â”€â”€â”€â”€â–¶â”‚   GitHub     â”‚â”€â”€â”€â”€â–¶â”‚   Cloud      â”‚                 â”‚
â”‚  â”‚   Push       â”‚     â”‚   Actions    â”‚     â”‚   Build      â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                              â”‚                     â”‚                         â”‚
â”‚                              â–¼                     â–¼                         â”‚
â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                       â”‚   Tests    â”‚        â”‚  Artifact  â”‚                  â”‚
â”‚                       â”‚  + Lint    â”‚        â”‚  Registry  â”‚                  â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```txt
â”œâ”€â”€ .github/                  # GitHub Actions workflows
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ tests.yaml        # Run tests on push/PR
â”‚       â”œâ”€â”€ linting.yaml      # Code linting with ruff
â”‚       â””â”€â”€ docker-build.yaml # Build and push Docker images
â”œâ”€â”€ configs/                  # Experiment configuration files
â”‚   â””â”€â”€ experiments/
â”‚       â””â”€â”€ default.yaml      # Default training config
â”œâ”€â”€ data/                     # Data directory (DVC tracked)
â”‚   â””â”€â”€ processed/            # Tokenized dataset
â”œâ”€â”€ dockerfiles/              # Docker configurations
â”‚   â”œâ”€â”€ api.dockerfile        # API server image
â”‚   â”œâ”€â”€ train.dockerfile      # Training image
â”‚   â””â”€â”€ evaluate.dockerfile   # Evaluation image
â”œâ”€â”€ docs/                     # MkDocs documentation
â”‚   â””â”€â”€ source/
â”œâ”€â”€ models/                   # Trained model checkpoints (DVC tracked)
â”œâ”€â”€ reports/                  # Exam report and figures
â”œâ”€â”€ src/ml_ops_assignment/    # Source code
â”‚   â”œâ”€â”€ api.py                # FastAPI application
â”‚   â”œâ”€â”€ data.py               # Data loading and preprocessing
â”‚   â”œâ”€â”€ evaluate.py           # Model evaluation script
â”‚   â”œâ”€â”€ model.py              # Model definition and training logic
â”‚   â”œâ”€â”€ train.py              # Training entry point
â”‚   â””â”€â”€ visualize.py          # Visualization utilities
â”œâ”€â”€ tests/                    # Unit tests
â”‚   â”œâ”€â”€ test_api.py           # API tests
â”‚   â”œâ”€â”€ test_data.py          # Data pipeline tests
â”‚   â””â”€â”€ test_model.py         # Model tests
â”œâ”€â”€ cloudbuild.yaml           # Google Cloud Build configuration
â”œâ”€â”€ pyproject.toml            # Project dependencies (uv)
â”œâ”€â”€ tasks.py                  # Invoke tasks for common commands
â””â”€â”€ README.md                 # This file
```

## Usage

### Training

```bash
# Preprocess data (if starting from scratch)
uv run invoke preprocess-data

# Train the model
uv run invoke train

# Train with Docker
uv run invoke docker-train
```

### Evaluation

```bash
# Evaluate on test set
uv run invoke evaluate --checkpoint models/model_final.pt

# Evaluate on validation set
uv run invoke evaluate --checkpoint models/model_final.pt --split validation
```

### API

```bash
# Start the API server locally
uv run invoke serve-api

# Make a prediction
curl -X 'POST' \
  'http://127.0.0.1:8000/predict' \
  -H 'Content-Type: application/json' \
  -d '{"text": "The quick brown fox jumps over the lazy dog."}'
```

### Testing

```bash
# Run all tests
uv run pytest tests/ -v

# Run tests with coverage
uv run pytest tests/ --cov=ml_ops_assignment --cov-report=term-missing
```

### Code Quality

```bash
# Format code
uv run ruff format .

# Lint code
uv run ruff check . --fix

# Run all pre-commit hooks
uv run pre-commit run --all-files
```

## Technology Stack

| Category | Tools |
|----------|-------|
| **ML Framework** | PyTorch 2.6.0, Transformers |
| **Model** | BERT-mini (prajjwal1/bert-mini) |
| **Data** | HuggingFace Datasets, DVC |
| **API** | FastAPI, Uvicorn, Pydantic |
| **Monitoring** | Prometheus, Weights & Biases |
| **Infrastructure** | Docker, Google Cloud Run, Cloud Build |
| **CI/CD** | GitHub Actions |
| **Code Quality** | Ruff, Mypy, Pre-commit |
| **Testing** | Pytest, Coverage |
| **Package Management** | uv |

## Dataset

The project uses the [OneStop English](https://huggingface.co/datasets/SetFit/onestop_english) dataset from HuggingFace, which contains English texts at three readability levels. The dataset is automatically split into:

- **Training**: 80% of the data
- **Validation**: 10% of the data
- **Test**: 10% of the data

## Acknowledgments

- Course instructors and TAs at DTU
- [MLOps Template](https://github.com/SkafteNicki/mlops_template) by Nicki Skafte
- [HuggingFace](https://huggingface.co/) for the dataset and transformers library
- [Astral](https://astral.sh/) for uv package manager

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
